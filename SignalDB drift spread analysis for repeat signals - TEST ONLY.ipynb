{"nbformat_minor": 0, "cells": [{"execution_count": 1, "cell_type": "code", "source": "import math\nimport string\nimport datetime\nimport numpy as np\nimport matplotlib.pyplot\nfrom pyspark.sql.types import *\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import Row\nimport pylab\n\n%matplotlib inline", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 8, "cell_type": "code", "source": "def config_softlayer_acct(name, auth_url, username, password):\n   prefix = \"fs.swift.service.\" + name\n   hconf = sc._jsc.hadoopConfiguration()\n   hconf.set(prefix + \".auth.url\", auth_url)\n   hconf.set(prefix + \".username\", username)\n   hconf.set(prefix + \".tenant\", username)\n   hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n   hconf.setInt(prefix + \".http.port\", 8080)\n   hconf.set(prefix + \".apikey\", password)\n   hconf.setBoolean(prefix + \".public\", True)\n   hconf.set(prefix + \".use.get.auth\", \"true\")\n   hconf.setBoolean(prefix + \".location-aware\", False)\n   hconf.set(prefix + \".password\", password)\n\n# THE ACCOUNT CONFIG HAS BEEN REMOVED- THIS PIPELINE WON'T WORK\n# TEST ONLY\nconfig_softlayer_acct(\"seti\",\"https://myserver\",\"myID\",\"mypassword\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 2, "cell_type": "code", "source": "# parse thedatafile to create the full seti database\n\ndef parseLineIgnoreHeaders(line):\n    words = []\n    if line.startswith('UniqueId'):\n        return words\n    words = line\n    return words\n\nsqlContext = SQLContext(sc)\n\ndata = sc.textFile(\"swift://setiSignalDB.seti/*\")\n\ndataForRowNames = sc.textFile(\"swift://setiSignalDB.seti/xaa\")\nrowNames = dataForRowNames.first().split(\"\\t\")\n\nrowNamesClear = []\n\n#We need to modify column names with \"/\" in it, because of issues when call these columns later\nfor name in rowNames:\n    if name.find(\"/\"):\n        rowNamesClear.append(name.replace(\"/\",\"\"))\n    else: \n        rowNamesClear.append(name)\n\ncleanData = data.map(lambda line:parseLineIgnoreHeaders(line)).filter(lambda words: len(words)>0)\nrowRDD = cleanData.map(lambda line:line.split(\"\\t\")).map(lambda d:(d[0],d[1],d[2],d[3],d[4],d[5],d[6],d[7],d[8],d[9],d[10],d[11],d[12],d[13],d[14],d[15],d[16],d[17],d[18],d[19],d[20],d[21],d[22]))\n\nfields = [StructField(field_name, StringType(), True) for field_name in rowNamesClear]\nschema = StructType(fields)\n\nexo_file = sqlContext.createDataFrame(rowRDD, schema)", "outputs": [{"ename": "Py4JJavaError", "evalue": "An error occurred while calling o32.partitions.\n: org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: Missing mandatory configuration option: fs.swift.service.seti.auth.url\n\tat org.apache.hadoop.fs.swift.http.RestClientBindings.copy(RestClientBindings.java:221)\n\tat org.apache.hadoop.fs.swift.http.RestClientBindings.bind(RestClientBindings.java:147)\n\tat org.apache.hadoop.fs.swift.http.SwiftRestClient.<init>(SwiftRestClient.java:481)\n\tat org.apache.hadoop.fs.swift.http.SwiftRestClient.getInstance(SwiftRestClient.java:1766)\n\tat org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore.initialize(SwiftNativeFileSystemStore.java:81)\n\tat org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem.initialize(SwiftNativeFileSystem.java:126)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:256)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:65)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:801)\n", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[1;32m<ipython-input-2-061e45b9078d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdataForRowNames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"swift://setiSignalDB.seti/xaa\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mrowNames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataForRowNames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mrowNamesClear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/local/src/bluemix_ipythonspark_141/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \"\"\"\n\u001b[1;32m-> 1295\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/local/src/bluemix_ipythonspark_141/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1245\u001b[0m         \"\"\"\n\u001b[0;32m   1246\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1247\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1248\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/local/src/bluemix_ipythonspark_141/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \"\"\"\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/local/src/bluemix_ipythonspark_141/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;32m/usr/local/src/bluemix_ipythonspark_141/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n", "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o32.partitions.\n: org.apache.hadoop.fs.swift.exceptions.SwiftConfigurationException: Missing mandatory configuration option: fs.swift.service.seti.auth.url\n\tat org.apache.hadoop.fs.swift.http.RestClientBindings.copy(RestClientBindings.java:221)\n\tat org.apache.hadoop.fs.swift.http.RestClientBindings.bind(RestClientBindings.java:147)\n\tat org.apache.hadoop.fs.swift.http.SwiftRestClient.<init>(SwiftRestClient.java:481)\n\tat org.apache.hadoop.fs.swift.http.SwiftRestClient.getInstance(SwiftRestClient.java:1766)\n\tat org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystemStore.initialize(SwiftNativeFileSystemStore.java:81)\n\tat org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem.initialize(SwiftNativeFileSystem.java:126)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2596)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:256)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:217)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:65)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:801)\n"], "output_type": "error"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# ... OR....\nexo_file = sqlContext.read.parquet(\"signalDB.parquet\")", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "\ndef deltaCalc(time):\n    dt = datetime.datetime.strptime(time,\"%Y-%m-%d %H:%M:%S\")\n            # the J2000.0 epoch start on January 1, 2000, 12:00 GMT = 4:00 PST\n    delta = dt - datetime.datetime.strptime(\"2000-01-01 04:00:00\", \"%Y-%m-%d %H:%M:%S\")\n    return float(delta.total_seconds() / (365.25*24*60*60))\n\nexo_rdd = exo_file.rdd.map(lambda p: Row(TgtId=p.TgtId, J2000_time=deltaCalc(p.Time),drift=float(p.DriftHzs)))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "doppler_fields = [StructField(\"J2000_time\",FloatType(), True),\n                  StructField(\"TgtId\",StringType(), True),\n                  StructField(\"drift\",FloatType(), True)\n                  ]\n\ndoppler_schema = StructType(doppler_fields)\nprint doppler_schema", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "# Register the doppler corrected DF as a table.\ncalcdf = sqlContext.createDataFrame(exo_rdd, doppler_schema)\ncalcdf.registerTempTable(\"calcdb\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "driftSpreadDF = sqlContext.sql(\"SELECT TgtId, MIN(drift) minimum_d, MAX (drift) maximum_d, MIN(J2000_time) minimum_time, MAX(J2000_time) maximum_time from calcdb GROUP BY TgtId\")\ndriftSpreadDF.count()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "#show results for export\ndriftSpreadDF.show(20000)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.9", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "name": "SignalDB drift spread analysis for repeat signals"}}